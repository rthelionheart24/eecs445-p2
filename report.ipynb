{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Preprocessing (See Appendix A for code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "i). <br />\n",
    "Mean: [124.495 118.847  95.293]<br />\n",
    "Std:  [62.754 59.597 62.425]<br /><br />\n",
    "ii). <br />\n",
    "Because we are training a model that is representative of the training dataset. This means that the model should be fit based on properties of the training data. We are simply applying the model to validation and testing data; so if we use their properties, it may cause overfitting. It is also te case that normalization puts everything on the same scale and this simplifies the learning process.\n",
    "\n",
    "### (b)\n",
    "![](Data_Preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Convolutional Neural Network (See Appendix B)\n",
    "\n",
    "### (a)\n",
    "For CNN, the number of learnable parameter (weights) can be calculated by (num_filters * filter_size * filter_size * num_channels) + num_filters <br />\n",
    "For fully connected layers, the number of weights is (input_size*output_size) + num_biases <br />\n",
    "\n",
    "So for layer 1, there are 16*5*5*3 + 16 = 1216 learnable parameters. <br />\n",
    "For layer 2, there are 64*5*5*16 + 64 = 25664 learnable parameters. <br />\n",
    "For layer 3, there are 8*5*5*64 + 6 = 12808 learnable parameters. <br />\n",
    "For the fully connected layer, there are 32*2 + 2 = 66 learnable parameters. <br />\n",
    "In total, there are 39754 parameters. <br />\n",
    "\n",
    "### (f)\n",
    "i). <br />\n",
    "One reason is that our the model is overfitting the training data so it won't perform well on the validation dataset. Another possible reason is that the the training, validation, and testing dataset aren't distributed similarly. So what the model learns from the training data can't be used to predict the validation data well. It is also possible that the amount of data we have isn't sufficient and this will lead to a model with poor performance. <br /><br />\n",
    "\n",
    "ii). <br />\n",
    "The model stopped training at epoch 11. With a patience of 10, the model would have to wait until epoch 16 to stop. Based on the training graphs, patience = 5 is a better choice because the graphs are all generally identical. However, patience = 5 takes fewer epoch to reach the performance. A higher patience might be better for training data that has a local minimum; if the patience is high, we won't stop at the local minimum and could potentially get a better model. <br /><br /><br />\n",
    "\n",
    "When patience = 5\n",
    "![](cnn_training_plot_5.png)\n",
    "When patience = 10\n",
    "![](cnn_training_plot_10.png)\n",
    "\n",
    "iii). <br />\n",
    "The new size is 64x2x2 = 256.\n",
    "\n",
    "|                \t| Epoch \t| Training AUROC \t| Validation AUROC \t|\n",
    "|:---------------:\t|:-------:\t|:----------------:\t|:------------------:\t|\n",
    "| 8 filters     \t| 6    \t    | 0.9867         \t| 0.9419            \t|\n",
    "| 64 filters    \t| 2     \t| 0.9777         \t| 0.923           \t|\n",
    "\n",
    "The performance actually decreases as we increase the number of filters. This could be because that the architecture we set up makes the model overfits; we are having more neurons then we need to correctly classify an image. It could also be the case that the added inputs introduce unnecessary noise to the fully connected layer, which can distrub the overall accuracy of the model. <br />\n",
    "\n",
    "### (g)\n",
    "i).\n",
    "\n",
    "|          \t| Training \t| Validation \t| Testing \t|\n",
    "|:----------:\t|:----------:\t|:------------:\t|:---------:\t|\n",
    "| Accuracy \t| 0.9667   \t| 0.9067       \t| 0.61    \t|\n",
    "| AUROC    \t| 0.9867   \t| 0.9419      \t| 0.6548  \t|\n",
    "\n",
    "ii). <br />\n",
    "Yes. We can see that the training accuracy is far better than the validation performance and it's similar for the AUROC score. So the model could potentially be overfitting. <br />\n",
    "\n",
    "iii). <br />\n",
    "We see that there the testing perforamnce is much wrose than the validation performance in both the accuracy and AUROC score. This could be because we are not splitting our data properly and the validation data has a lot more label belonging to golden retriever while the testing data has a lot more labels of Collies. Therefore, the model will perform poorly on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualizing what the CNN has learned\n",
    "\n",
    "### (a) <br /> \n",
    "\n",
    "$$\\alpha_1 = \\frac{1}{16} \\sum_{i}\\sum_{j} \\frac{\\partial dy}{\\partial A'_{ij}} = 1+1+2+1+1+2+1+1-1+1-2-2=\\frac{3}{16}$$\n",
    "$$\\alpha_2 = \\frac{1}{16} \\sum_{i}\\sum_{j} \\frac{\\partial dy}{\\partial A'_{ij}} = 1+1+1+1+2+2+2+2+2+2+1-1-1-1=\\frac{7}{16}$$\n",
    "\n",
    "$$L'=ReLU(\\sum_{k}\\alpha'_kA^{(k)}) = ReLU(\\frac{3}{16}\\begin{bmatrix} 1 & 1 & 2 & 1 \\\\ 1 & 2 & 1 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 1 & -2 & -2 \\end{bmatrix} + \\frac{7}{16}\\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 2 & 2 & 2 \\\\ 2 & 2 & 1 & 0 \\\\ -1 & -1 & -1 & 0 \\end{bmatrix}) = \\frac{1}{16} \\begin{bmatrix} 10 & 10 & 13 & 10 \\\\ 17 & 20 & 17 & 14 \\\\ 14 & 17 & 7 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "### (b) <br /> \n",
    "The CNN appears to be using green pixels to identify between Golden Retrievers and Collies.\n",
    "### (C) <br />\n",
    "Yes, it confirms the hypothesis. It seems like the model relies heavily on green pixels to identify between Golden Retrievers and Collies. However, we as humans know that this is not the distinctive feature between the two types of dogs. This shows that the model has a strong bias toward green color, and it further implies that the green is over-represented in the training data and our model is biased and cannot perform as well on the testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Transfer Learning & Data Augmentation\n",
    "\n",
    "### 4.1 Transfer Learning (See Appendix C)\n",
    "\n",
    "#### (c)\n",
    "![](source_training_plot.png)\n",
    "Epoch 10 has the lowest validation loss with a value of 1.8664. <br />\n",
    "\n",
    "#### (d)\n",
    "![](conf_matrix.png) <br />\n",
    "The classifier is the most accurate when predicting for Samoyed and it is the least accurate when predicting for Syberian Husky. This might be because we the training data we have is mislabeled and most Syberian Husky use Samoyed's label. This means that the classifier will treat most Syberian Husky as Samoyed and classify them correctly. Since Samoyed and Siberian Husky have very similar features, this strengthens the model's connection with Samoyed. On the other hand, the classifier doesn't have enough information to classify Syberian Husky because the train dataset doesn't contain many samples of it. This will result in very low accuracy for Syberian Husky. <br />\n",
    "\n",
    "#### (f)\n",
    "|                                                                  \t|        \t|  AUROC \t|        \t|\n",
    "|:----------------------------------------------------------------:\t|:------:\t|:------:\t|:------:\t|\n",
    "|                                                                  \t|  TRAIN \t|   VAL  \t|  TEST  \t|\n",
    "|            Freeze all CONV layers (Fine_tune FC layer)           \t| 0.8822 \t| 0.8832 \t| 0.826 \t|\n",
    "| Freeze first two CONV layers (Fine-tune last CONV and FC layers) \t| 0.9865 \t| 0.9102 \t| 0.7932 \t|\n",
    "|   Freeze first CONV layer (Fine-tune last 2 conv, and fc layer)  \t| 0.9904 \t| 0.922 \t| 0.788 \t|\n",
    "|              Freeze no layers (Fine-tune all layers)             \t| 0.9905 \t| 0.9262 \t| 0.7576 \t|\n",
    "|    No pretraining or Transfer Learning (Section 2 performance)   \t| 0.9867 \t|  0.9419 \t| 0.6548 \t|\n",
    "\n",
    "Transfer learning helps significantly and the source task we used is very helpful because we witness a huge imporvement in testing performance. Freezing all layers results in a lot more epoches than when we freeze a subset of the layers. This is because without convolutional layers, we are not filtering our data. So our fully connected layer will receive many many more inputs. This means that it will take a lot longer to train the classifier and many epoches are taken before we reach a good performance. Nevertheless, freezing all convolutional layer results in a much better performance; this is because we are preserving all learnable features in our dataset. There will always be information loss during the filtering stage. The negative consequence of having no convolutional layer is high  computation cost. <br />\n",
    "\n",
    "### 4.2 Data Augmentation (See Appendix D and E)\n",
    "\n",
    "#### (b)\n",
    "\n",
    "|                                         \t|        \t|  AUROC \t|        \t|\n",
    "|:---------------------------------------:\t|:------:\t|:------:\t|:------:\t|\n",
    "|                                         \t|  TRAIN \t|   VAL  \t|  TEST  \t|\n",
    "|         Rotation (Keep original)        \t| 0.9812    | 0.934    | 0.6584    |\n",
    "|        Grayscale (keep original)        \t| 0.9915 \t| 0.9275 \t| 0.7404 \t|\n",
    "|       Grayscale (discard original)      \t| 0.9672 \t| 0.8025 \t| 0.7776 \t|\n",
    "| No augmentation (section 2 performance) \t| 0.9867 \t|  0.9419 \t| 0.6548 \t|\n",
    "\n",
    "Training plot of Rotation (Keep original) <br />\n",
    "![](rotation_training_plot.png) <br />\n",
    "\n",
    "Training plot of Grayscale (keep original) <br />\n",
    "![](grayscale_true_training_plot.png) <br />\n",
    "\n",
    "Training plot of Grayscale (discard original) <br />\n",
    "![](grayscale_false_training_plot.png) <br />\n",
    "\n",
    "\n",
    "#### (c)\n",
    "When we apply rotation, the performance is roughly the same compared to when there is no augmentation. This might be because the validation and testing dataset doesn't contain many rotated objects. It's also possible that the rotation results in loss of information and the model is underfitted. One other possibility is that the augmentated samples aren't sufficient for the model to learn about potential rotated objects. <br />\n",
    "When we apply grayscale, either keeping or discarding the original images, we witness a great increase in testing performance. This could be due to the fact that grayscale essentially reduces the noise and thus prevents overfitting. However, one exception is that when we discard the original images, the model does worse on the validation data. This may be due to loss of information from the original dataset and insufficient training samples. However, the testing performance remains better that when we don't do any augmentation because of noise reduction. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge (See appendix F)\n",
    "\n",
    "Regularization: I decided to apply Ridge Regularization by setting the weight decay to 0.01. I also added a dropout feature with a probability of 0.5. Both of these features help reduce overfitting. I tried using l1 regularization, but it doesn't seem to help with the performance. <br /> <br />\n",
    "\n",
    "Feature selection: I didn't implement feature selection because I lack understanding of the dataset. However, if we know the dataset enough, for example, the dataset is ranked from the least blurry to the most blurry images, we can use only the top 50% of the dataset because they are better training data. <br /> <br />\n",
    "\n",
    "Model architecture: Initially, I tried using a VGG16 model. I realized that the dataset we are given (64x64) isn't really compatible with VGG16 (requires 224x224). For this reason, I cannot implement 5 convolutonal layers. Eventually, I decided to use a modify the archietecture the appendix provides, with three convolutional layer. The first one has 32 filters, the second one has 64 filter, and the third one has 32 filters. There are two max pooling layers between these three convolutional layers. After that, I set up one fully connected layers that takes in 128 inputs. <br /><br />\n",
    "\n",
    "Hyperparameters: I decide to use a patience of 10 because our learning rate is reletively high. A high learning rate means that it is likely that our loss oscillates between the left and right side of the minimum. Therefore, a higher patience allows extra time for the model to converge to a potential global minimum. <br /><br />\n",
    "\n",
    "Transfer learning: I decide to use transfer learning to boost the performance of my model. As we can see from previous parts, using transfer learning could help improve our model's accuracy when we also freeze some layers of the CNN. I modified the source model so that it is compatible with the challenge model in terms of the architecture. After some experimentation, I chose to freeze all convolutional layers except for the fully connected layer because it gives the best performance. This might be because we are perserving more features than when we applied filters on images, which means that our model can learn more about the dataset. <br /><br />\n",
    "\n",
    "Data augmentation: I decide not to apply any data augmentation because it results in really bad performance compared to when there is no augmentation. I think this might be because the mixture of using transfer learning and data agumentation results in the model being overfit, but there could be other reasons behind this. We can potentially investigate the how these two methods affect each other in the future.  <br /><br />\n",
    "\n",
    "Model evaluation: I decide to stick with validation loss as the evaluation metric because using any training metric will likely result in overfitting. Using validation accuracy or AUROC might be good. However, we don't have enough insight of the dataset, and if it is heavily imbalanced, using neither of these metrics gives an accurate description of the performance. So validation loss remains the best metric. <br /><br />\n",
    "\n",
    "I disabled CUDA acceleration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Dogs Dataset\n",
    "    Class wrapper for interfacing with the dataset of dog images\n",
    "    Usage: python dataset.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from imageio import imread\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils import config\n",
    "\n",
    "\n",
    "def get_train_val_test_loaders(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoaders for train, val and test splits.\n",
    "\n",
    "    Any keyword arguments are forwarded to the DogsDataset constructor.\n",
    "    \"\"\"\n",
    "    tr, va, te, _ = get_train_val_test_datasets(task, **kwargs)\n",
    "\n",
    "    tr_loader = DataLoader(tr, batch_size=batch_size, shuffle=True)\n",
    "    va_loader = DataLoader(va, batch_size=batch_size, shuffle=False)\n",
    "    te_loader = DataLoader(te, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return tr_loader, va_loader, te_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_challenge(task, batch_size, **kwargs):\n",
    "    \"\"\"Return DataLoader for challenge dataset.\n",
    "\n",
    "    Any keyword arguments are forwarded to the DogsDataset constructor.\n",
    "    \"\"\"\n",
    "    tr = DogsDataset(\"train\", task, **kwargs)\n",
    "    ch = DogsDataset(\"challenge\", task, **kwargs)\n",
    "\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    ch.X = standardizer.transform(ch.X)\n",
    "\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "    ch.X = ch.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    ch_loader = DataLoader(ch, batch_size=batch_size, shuffle=False)\n",
    "    return ch_loader, tr.get_semantic_label\n",
    "\n",
    "\n",
    "def get_train_val_test_datasets(task=\"default\", **kwargs):\n",
    "    \"\"\"Return DogsDatasets and image standardizer.\n",
    "\n",
    "    Image standardizer should be fit to train data and applied to all splits.\n",
    "    \"\"\"\n",
    "    tr = DogsDataset(\"train\", task, **kwargs)\n",
    "    va = DogsDataset(\"val\", task, **kwargs)\n",
    "    te = DogsDataset(\"test\", task, **kwargs)\n",
    "\n",
    "    # Resize\n",
    "    # We don't resize images, but you may want to experiment with resizing\n",
    "    # images to be smaller for the challenge portion. How might this affect\n",
    "    # your training?\n",
    "    # tr.X = resize(tr.X)\n",
    "    # va.X = resize(va.X)\n",
    "    # te.X = resize(te.X)\n",
    "\n",
    "    # Standardize\n",
    "    standardizer = ImageStandardizer()\n",
    "    standardizer.fit(tr.X)\n",
    "    tr.X = standardizer.transform(tr.X)\n",
    "    va.X = standardizer.transform(va.X)\n",
    "    te.X = standardizer.transform(te.X)\n",
    "\n",
    "    # Transpose the dimensions from (N,H,W,C) to (N,C,H,W)\n",
    "    tr.X = tr.X.transpose(0, 3, 1, 2)\n",
    "    va.X = va.X.transpose(0, 3, 1, 2)\n",
    "    te.X = te.X.transpose(0, 3, 1, 2)\n",
    "\n",
    "    return tr, va, te, standardizer\n",
    "\n",
    "\n",
    "def resize(X):\n",
    "    \"\"\"Resize the data partition X to the size specified in the config file.\n",
    "\n",
    "    Use bicubic interpolation for resizing.\n",
    "\n",
    "    Returns:\n",
    "        the resized images as a numpy array.\n",
    "    \"\"\"\n",
    "    image_dim = config(\"image_dim\")\n",
    "    image_size = (image_dim, image_dim)\n",
    "    resized = []\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = Image.fromarray(X[i]).resize(image_size, resample=2)\n",
    "        resized.append(xi)\n",
    "    resized = [np.asarray(im) for im in resized]\n",
    "\n",
    "    return resized\n",
    "\n",
    "\n",
    "class ImageStandardizer(object):\n",
    "    \"\"\"Standardize a batch of images to mean 0 and variance 1.\n",
    "\n",
    "    The standardization should be applied separately to each channel.\n",
    "    The mean and standard deviation parameters are computed in `fit(X)` and\n",
    "    applied using `transform(X)`.\n",
    "\n",
    "    X has shape (N, image_height, image_width, color_channel)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize mean and standard deviations to None.\"\"\"\n",
    "        super().__init__()\n",
    "        self.image_mean = None\n",
    "        self.image_std = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Calculate per-channel mean and standard deviation from dataset X.\"\"\"\n",
    "        # TODO: Complete this function\n",
    "        print(X.shape)\n",
    "        self.image_mean = np.mean(X, axis=(0,1,2))\n",
    "        self.image_std = np.std(X, axis=(0,1,2))\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Return standardized dataset given dataset X.\"\"\"\n",
    "        # TODO: Complete this function\n",
    "        X_transformed = (X - self.image_mean) / self.image_std\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "class DogsDataset(Dataset):\n",
    "    \"\"\"Dataset class for dog images.\"\"\"\n",
    "\n",
    "    def __init__(self, partition, task=\"target\", augment=False):\n",
    "        \"\"\"Read in the necessary data from disk.\n",
    "\n",
    "        For parts 2, 3 and data augmentation, `task` should be \"target\".\n",
    "        For source task of part 4, `task` should be \"source\".\n",
    "\n",
    "        For data augmentation, `augment` should be True.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if partition not in [\"train\", \"val\", \"test\", \"challenge\"]:\n",
    "            raise ValueError(\"Partition {} does not exist\".format(partition))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        torch.manual_seed(42)\n",
    "        random.seed(42)\n",
    "        self.partition = partition\n",
    "        self.task = task\n",
    "        self.augment = augment\n",
    "        # Load in all the data we need from disk\n",
    "        if task == \"target\" or task == \"source\":\n",
    "            self.metadata = pd.read_csv(config(\"csv_file\"))\n",
    "        if self.augment:\n",
    "            print(\"Augmented\")\n",
    "            self.metadata = pd.read_csv(config(\"augmented_csv_file\"))\n",
    "        self.X, self.y = self._load_data()\n",
    "\n",
    "        self.semantic_labels = dict(\n",
    "            zip(\n",
    "                self.metadata[self.metadata.task == self.task][\"numeric_label\"],\n",
    "                self.metadata[self.metadata.task == self.task][\"semantic_label\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return size of dataset.\"\"\"\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (image, label) pair at index `idx` of dataset.\"\"\"\n",
    "        return torch.from_numpy(self.X[idx]).float(), torch.tensor(self.y[idx]).long()\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"Load a single data partition from file.\"\"\"\n",
    "        print(\"loading %s...\" % self.partition)\n",
    "\n",
    "        df = self.metadata[\n",
    "            (self.metadata.task == self.task)\n",
    "            & (self.metadata.partition == self.partition)\n",
    "        ]\n",
    "\n",
    "        if self.augment:\n",
    "            path = config(\"augmented_image_path\")\n",
    "        else:\n",
    "            path = config(\"image_path\")\n",
    "\n",
    "        X, y = [], []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row[\"numeric_label\"]\n",
    "            image = imread(os.path.join(path, row[\"filename\"]))\n",
    "            X.append(image)\n",
    "            y.append(row[\"numeric_label\"])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    def get_semantic_label(self, numeric_label):\n",
    "        \"\"\"Return the string representation of the numeric class label.\n",
    "\n",
    "        (e.g., the numberic label 1 maps to the semantic label 'miniature_poodle').\n",
    "        \"\"\"\n",
    "        return self.semantic_labels[numeric_label]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.set_printoptions(precision=3)\n",
    "    tr, va, te, standardizer = get_train_val_test_datasets(task=\"target\", augment=False)\n",
    "    print(\"Train:\\t\", len(tr.X))\n",
    "    print(\"Val:\\t\", len(va.X))\n",
    "    print(\"Test:\\t\", len(te.X))\n",
    "    print(\"Mean:\", standardizer.image_mean)\n",
    "    print(\"Std: \", standardizer.image_std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Target CNN\n",
    "    Constructs a pytorch model for a convolutional neural network\n",
    "    Usage: from model.target import target\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from utils import config\n",
    "\n",
    "from math import floor\n",
    "\n",
    "\n",
    "class Target(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: define each layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.fc_1 = nn.Linear(in_features=32, out_features=2)\n",
    "        ##\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        for conv in [self.conv1, self.conv2, self.conv3]:\n",
    "            C_in = conv.weight.size(1)\n",
    "            nn.init.normal_(conv.weight, 0.0, 1 / sqrt(5 * 5 * C_in))\n",
    "            nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "        # TODO: initialize the parameters for [self.fc_1]\n",
    "\n",
    "        nn.init.normal_(self.fc_1.weight, 0.0, 1 / sqrt(32))\n",
    "        nn.init.constant_(self.fc_1.bias, 0.0)\n",
    "        ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" You may optionally use the x.shape variables below to resize/view the size of\n",
    "            the input matrix at different points of the forward pass\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # TODO: forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 32)\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        ##\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Train Target\n",
    "    Train a convolutional neural network to classify images.\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_target.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def freeze_layers(model, num_layers=0):\n",
    "    \"\"\"Stop tracking gradients on selected layers.\"\"\"\n",
    "    # TODO: modify model with the given layers frozen\n",
    "    #      e.g. if num_layers=2, freeze CONV1 and CONV2     \n",
    "    #      Hint: https://pytorch.org/docs/master/notes/autograd.html\n",
    "\n",
    "    track = num_layers * 2\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if track == 0:\n",
    "            break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        track -= 1\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "\n",
    "def train(tr_loader, va_loader, te_loader, model, model_name, num_layers=0):\n",
    "    \"\"\"Train transfer learning model.\"\"\"\n",
    "    # TODO: define loss function, and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "    #\n",
    "\n",
    "    print(\"Loading target model with\", num_layers, \"layers frozen\")\n",
    "    model, start_epoch, stats = restore_checkpoint(model, model_name)\n",
    "\n",
    "    axes = utils.make_training_plot(\"Target Training\")\n",
    "\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,\n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        include_test=True,\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: patience for early stopping\n",
    "    patience = 5\n",
    "    curr_count_to_patience = 0\n",
    "    #\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            include_test=True,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, model_name, stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    # Keep plot open\n",
    "    utils.save_tl_training_plot(num_layers)\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train transfer learning model and display training plots.\n",
    "\n",
    "    Train four different models with {0, 1, 2, 3} layers frozen.\n",
    "    \"\"\"\n",
    "    # data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"target\",\n",
    "        batch_size=config(\"target.batch_size\"),\n",
    "    )\n",
    "\n",
    "    freeze_none = Target()\n",
    "    print(\"Loading source...\")\n",
    "    freeze_none, _, _ = restore_checkpoint(\n",
    "        freeze_none, config(\"source.checkpoint\"), force=True, pretrain=True\n",
    "    )\n",
    "\n",
    "    freeze_one = copy.deepcopy(freeze_none)\n",
    "    freeze_two = copy.deepcopy(freeze_none)\n",
    "    freeze_three = copy.deepcopy(freeze_none)\n",
    "\n",
    "    freeze_layers(freeze_one, 1)\n",
    "    freeze_layers(freeze_two, 2)\n",
    "    freeze_layers(freeze_three, 3)\n",
    "\n",
    "    train(tr_loader, va_loader, te_loader, freeze_none, \"./checkpoints/target0/\", 0)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_one, \"./checkpoints/target1/\", 1)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_two, \"./checkpoints/target2/\", 2)\n",
    "    train(tr_loader, va_loader, te_loader, freeze_three, \"./checkpoints/target3/\", 3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022  - Project 2\n",
    "\n",
    "Helper file for common training functions.\n",
    "\"\"\"\n",
    "\n",
    "from utils import config\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn import metrics\n",
    "import utils\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count number of learnable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, epoch, checkpoint_dir, stats):\n",
    "    \"\"\"Save a checkpoint file to `checkpoint_dir`.\"\"\"\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"stats\": stats,\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(epoch))\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "def check_for_augmented_data(data_dir):\n",
    "    \"\"\"Ask to use augmented data if `augmented_dogs.csv` exists in the data directory.\"\"\"\n",
    "    if \"augmented_dogs.csv\" in os.listdir(data_dir):\n",
    "        print(\"Augmented data found, would you like to use it? y/n\")\n",
    "        print(\">> \", end=\"\")\n",
    "        rep = str(input())\n",
    "        return rep == \"y\"\n",
    "    return False\n",
    "\n",
    "\n",
    "def restore_checkpoint(model, checkpoint_dir, cuda=False, force=False, pretrain=False):\n",
    "    \"\"\"Restore model from checkpoint if it exists.\n",
    "\n",
    "    Returns the model and the current epoch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cp_files = [\n",
    "            file_\n",
    "            for file_ in os.listdir(checkpoint_dir)\n",
    "            if file_.startswith(\"epoch=\") and file_.endswith(\".checkpoint.pth.tar\")\n",
    "        ]\n",
    "    except FileNotFoundError:\n",
    "        cp_files = None\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not cp_files:\n",
    "        print(\"No saved model parameters found\")\n",
    "        if force:\n",
    "            raise Exception(\"Checkpoint not found\")\n",
    "        else:\n",
    "            return model, 0, []\n",
    "\n",
    "    # Find latest epoch\n",
    "    for i in itertools.count(1):\n",
    "        if \"epoch={}.checkpoint.pth.tar\".format(i) in cp_files:\n",
    "            epoch = i\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if not force:\n",
    "        print(\n",
    "            \"Which epoch to load from? Choose in range [0, {}].\".format(epoch),\n",
    "            \"Enter 0 to train from scratch.\",\n",
    "        )\n",
    "        print(\">> \", end=\"\")\n",
    "        inp_epoch = int(input())\n",
    "        if inp_epoch not in range(epoch + 1):\n",
    "            raise Exception(\"Invalid epoch number\")\n",
    "        if inp_epoch == 0:\n",
    "            print(\"Checkpoint not loaded\")\n",
    "            clear_checkpoint(checkpoint_dir)\n",
    "            return model, 0, []\n",
    "    else:\n",
    "        print(\"Which epoch to load from? Choose in range [1, {}].\".format(epoch))\n",
    "        inp_epoch = int(input())\n",
    "        if inp_epoch not in range(1, epoch + 1):\n",
    "            raise Exception(\"Invalid epoch number\")\n",
    "\n",
    "    filename = os.path.join(\n",
    "        checkpoint_dir, \"epoch={}.checkpoint.pth.tar\".format(inp_epoch)\n",
    "    )\n",
    "\n",
    "    print(\"Loading from checkpoint {}?\".format(filename))\n",
    "\n",
    "    if cuda:\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        # Load GPU model on CPU\n",
    "        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
    "\n",
    "    try:\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        stats = checkpoint[\"stats\"]\n",
    "        if pretrain:\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        print(\n",
    "            \"=> Successfully restored checkpoint (trained for {} epochs)\".format(\n",
    "                checkpoint[\"epoch\"]\n",
    "            )\n",
    "        )\n",
    "    except:\n",
    "        print(\"=> Checkpoint not successfully restored\")\n",
    "        raise\n",
    "\n",
    "    return model, inp_epoch, stats\n",
    "\n",
    "\n",
    "def clear_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Remove checkpoints in `checkpoint_dir`.\"\"\"\n",
    "    filelist = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth.tar\")]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(checkpoint_dir, f))\n",
    "\n",
    "    print(\"Checkpoint successfully removed\")\n",
    "\n",
    "\n",
    "def early_stopping(stats, curr_count_to_patience, global_min_loss):\n",
    "    \"\"\"Calculate new patience and validation loss.\n",
    "\n",
    "    Increment curr_count_to_patience by one if new loss is not less than global_min_loss\n",
    "    Otherwise, update global_min_loss with the current val loss\n",
    "\n",
    "    Returns: new values of curr_count_to_patience and global_min_loss\n",
    "    \"\"\"\n",
    "    # TODO implement early stopping\n",
    "    if stats[-1][1] >= global_min_loss:\n",
    "        curr_count_to_patience += 1\n",
    "    else:\n",
    "        curr_count_to_patience = 0\n",
    "        global_min_loss = stats[-1][1]\n",
    "\n",
    "    #\n",
    "    return curr_count_to_patience, global_min_loss\n",
    "\n",
    "\n",
    "def evaluate_epoch(\n",
    "    axes,\n",
    "    tr_loader,\n",
    "    val_loader,\n",
    "    te_loader,\n",
    "    model,\n",
    "    criterion,\n",
    "    epoch,\n",
    "    stats,\n",
    "    include_test=False,\n",
    "    update_plot=True,\n",
    "    multiclass=False,\n",
    "):\n",
    "    \"\"\"Evaluate the `model` on the train and validation set.\"\"\"\n",
    "\n",
    "    def _get_metrics(loader):\n",
    "        y_true, y_pred, y_score = [], [], []\n",
    "        correct, total = 0, 0\n",
    "        running_loss = []\n",
    "        for X, y in loader:\n",
    "            with torch.no_grad():\n",
    "                output = model(X)\n",
    "                predicted = predictions(output.data)\n",
    "                y_true.append(y)\n",
    "                y_pred.append(predicted)\n",
    "                if not multiclass:\n",
    "                    y_score.append(softmax(output.data, dim=1)[:, 1])\n",
    "                else:\n",
    "                    y_score.append(softmax(output.data, dim=1))\n",
    "                total += y.size(0)\n",
    "                correct += (predicted == y).sum().item()\n",
    "                running_loss.append(criterion(output, y).item())\n",
    "        y_true = torch.cat(y_true)\n",
    "        y_pred = torch.cat(y_pred)\n",
    "        y_score = torch.cat(y_score)\n",
    "        loss = np.mean(running_loss)\n",
    "        acc = correct / total\n",
    "        if not multiclass:\n",
    "            auroc = metrics.roc_auc_score(y_true, y_score)\n",
    "        else:\n",
    "            auroc = metrics.roc_auc_score(y_true, y_score, multi_class=\"ovo\")\n",
    "        return acc, loss, auroc\n",
    "\n",
    "    train_acc, train_loss, train_auc = _get_metrics(tr_loader)\n",
    "    val_acc, val_loss, val_auc = _get_metrics(val_loader)\n",
    "\n",
    "    stats_at_epoch = [\n",
    "        val_acc,\n",
    "        val_loss,\n",
    "        val_auc,\n",
    "        train_acc,\n",
    "        train_loss,\n",
    "        train_auc,\n",
    "    ]\n",
    "    if include_test:\n",
    "        stats_at_epoch += list(_get_metrics(te_loader))\n",
    "\n",
    "    stats.append(stats_at_epoch)\n",
    "    utils.log_training(epoch, stats)\n",
    "    if update_plot:\n",
    "        utils.update_training_plot(axes, epoch, stats)\n",
    "\n",
    "\n",
    "def train_epoch(data_loader, model, criterion, optimizer):\n",
    "    \"\"\"Train the `model` for one epoch of data from `data_loader`.\n",
    "\n",
    "    Use `optimizer` to optimize the specified `criterion`\n",
    "    \"\"\"\n",
    "    for i, (X, y) in enumerate(data_loader):\n",
    "        # TODO implement training steps\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictions(logits):\n",
    "    \"\"\"Determine predicted class index given logits.\n",
    "\n",
    "    Returns:\n",
    "        the predicted class output as a PyTorch Tensor\n",
    "    \"\"\"\n",
    "    # TODO implement predictions\n",
    "    pred = np.argmax(logits, axis=1)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Source CNN\n",
    "    Constructs a pytorch model for a convolutional neural network\n",
    "    Usage: from model.source import Source\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from utils import config\n",
    "\n",
    "\n",
    "class Source(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: define each layer\n",
    "        # self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(\n",
    "        #     5, 5), stride=(2, 2), padding=2)\n",
    "        # self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        # self.conv2 = nn.Conv2d(in_channels=16, out_channels=64, kernel_size=(\n",
    "        #     5, 5), stride=(2, 2), padding=2)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=(\n",
    "        #     5, 5), stride=(2, 2), padding=2)\n",
    "        # self.fc1 = nn.Linear(in_features=32, out_features=8)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.fc1 = nn.Linear(in_features=128, out_features=8)\n",
    "        ##\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        torch.manual_seed(42)\n",
    "        for conv in [self.conv1, self.conv2, self.conv3]:\n",
    "            C_in = conv.weight.size(1)\n",
    "            nn.init.normal_(conv.weight, 0.0, 1 / sqrt(5 * 5 * C_in))\n",
    "            nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "        # TODO: initialize the parameters for [self.fc1]\n",
    "        nn.init.normal_(self.fc1.weight, 0.0, 1 / sqrt(128))\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        \n",
    "        ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" You may optionally use the x.shape variables below to resize/view the size of\n",
    "            the input matrix at different points of the forward pass\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # TODO: forward pass\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        ##\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Train Source CNN\n",
    "    Train a convolutional neural network to classify images.\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python3 train_source.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.source import Source\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train source model on multiclass data.\"\"\"\n",
    "    # Data loaders\n",
    "    tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "        task=\"source\",\n",
    "        batch_size=config(\"source.batch_size\"),\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = Source()\n",
    "\n",
    "    # TODO: define loss function, and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.01, lr=1e-3)\n",
    "    #\n",
    "\n",
    "    print(\"Number of float-valued parameters:\", count_parameters(model))\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading source...\")\n",
    "    model, start_epoch, stats = restore_checkpoint(model, config(\"source.checkpoint\"))\n",
    "\n",
    "    axes = utils.make_training_plot(\"Source Training\")\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,\n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        multiclass=True,\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: patience for early stopping\n",
    "    patience = 10\n",
    "    curr_count_to_patience = 0\n",
    "    #\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            multiclass=True,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, config(\"source.checkpoint\"), stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    # Save figure and keep plot open\n",
    "    print(\"Finished Training\")\n",
    "    utils.save_source_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022  - Project 2\n",
    "Train CNN\n",
    "    Train a convolutional neural network to classify images\n",
    "    Periodically output training information, and saves model checkpoints\n",
    "    Usage: python train_cnn.py\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train CNN and show training plots.\"\"\"\n",
    "    # Data loaders\n",
    "    if check_for_augmented_data(\"./data\"):\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\", batch_size=config(\"target.batch_size\"), augment=True\n",
    "        )\n",
    "    else:\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"target.batch_size\"),\n",
    "        )\n",
    "    # Model\n",
    "    model = Target()\n",
    "\n",
    "    # TODO: define loss function, and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-3)\n",
    "    #\n",
    "\n",
    "    print(\"Number of float-valued parameters:\", count_parameters(model))\n",
    "\n",
    "    # Attempts to restore the latest checkpoint if exists\n",
    "    print(\"Loading cnn...\")\n",
    "    model, start_epoch, stats = restore_checkpoint(model, config(\"target.checkpoint\"))\n",
    "\n",
    "    axes = utils.make_training_plot()\n",
    "\n",
    "    # Evaluate the randomly initialized model\n",
    "    evaluate_epoch(\n",
    "        axes, tr_loader, va_loader, te_loader, model, criterion, start_epoch, stats\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: define patience for early stopping\n",
    "    patience = 5\n",
    "    curr_count_to_patience = 0\n",
    "    #\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes, tr_loader, va_loader, te_loader, model, criterion, epoch + 1, stats\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, config(\"target.checkpoint\"), stats)\n",
    "\n",
    "        # update early stopping parameters\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "\n",
    "        epoch += 1\n",
    "    print(\"Finished Training\")\n",
    "    # Save figure and keep plot open\n",
    "    utils.save_cnn_training_plot()\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022  - Project 2\n",
    "\n",
    "Script to create an augmented dataset.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "from imageio import imread, imwrite\n",
    "\n",
    "\n",
    "def Rotate(deg=20):\n",
    "    \"\"\"Return function to rotate image.\"\"\"\n",
    "\n",
    "    def _rotate(img):\n",
    "        \"\"\"Rotate a random amount in the range (-deg, deg).\n",
    "\n",
    "        Keep the dimensions the same and fill any missing pixels with black.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return rotate(\n",
    "            input=img,\n",
    "            angle=np.random.randint(-deg, deg),\n",
    "            reshape=False\n",
    "        )\n",
    "\n",
    "    return _rotate\n",
    "\n",
    "\n",
    "def Grayscale():\n",
    "    \"\"\"Return function to grayscale image.\"\"\"\n",
    "\n",
    "    def _grayscale(img):\n",
    "        \"\"\"Return 3-channel grayscale of image.\n",
    "\n",
    "        Compute grayscale values by taking average across the three channels.\n",
    "\n",
    "        Round to the nearest integer.\n",
    "\n",
    "        :img: H x W x C numpy array\n",
    "        :returns: H x W x C numpy array\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        avg = np.round(img.mean(axis=-1), 0).astype(np.uint8)\n",
    "        avg = np.stack([avg] * 3, axis=-1)\n",
    "        return avg\n",
    "\n",
    "    return _grayscale\n",
    "\n",
    "\n",
    "def augment(filename, transforms, n=1, original=True):\n",
    "    \"\"\"Augment image at filename.\n",
    "\n",
    "    :filename: name of image to be augmented\n",
    "    :transforms: List of image transformations\n",
    "    :n: number of augmented images to save\n",
    "    :returns: a list of augmented images, where the first image is the original\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting {filename}\")\n",
    "    img = imread(filename)\n",
    "    res = [img] if original else []\n",
    "    for i in range(n):\n",
    "        new = img\n",
    "        for transform in transforms:\n",
    "            new = transform(new)\n",
    "        res.append(new)\n",
    "    return res\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    \"\"\"Create augmented dataset.\"\"\"\n",
    "    reader = csv.DictReader(open(args.input, \"r\"), delimiter=\",\")\n",
    "    writer = csv.DictWriter(\n",
    "        open(f\"{args.datadir}/augmented_dogs.csv\", \"w\"),\n",
    "        fieldnames=[\"filename\", \"semantic_label\", \"partition\", \"numeric_label\", \"task\"],\n",
    "    )\n",
    "    augment_partitions = set(args.partitions)\n",
    "\n",
    "    # TODO: change `augmentations` to specify which augmentations to apply\n",
    "    augmentations = [Grayscale()]\n",
    "\n",
    "    writer.writeheader()\n",
    "    os.makedirs(f\"{args.datadir}/augmented/\", exist_ok=True)\n",
    "    for f in glob.glob(f\"{args.datadir}/augmented/*\"):\n",
    "        print(f\"Deleting {f}\")\n",
    "        os.remove(f)\n",
    "    for row in reader:\n",
    "        if row[\"partition\"] not in augment_partitions:\n",
    "            imwrite(\n",
    "                f\"{args.datadir}/augmented/{row['filename']}\",\n",
    "                imread(f\"{args.datadir}/images/{row['filename']}\"),\n",
    "            )\n",
    "            writer.writerow(row)\n",
    "            continue\n",
    "        imgs = augment(\n",
    "            f\"{args.datadir}/images/{row['filename']}\",\n",
    "            augmentations,\n",
    "            n=1,\n",
    "            original=False,  # TODO: change to False to exclude original image.\n",
    "        )\n",
    "        for i, img in enumerate(imgs):\n",
    "            fname = f\"{row['filename'][:-4]}_aug_{i}.png\"\n",
    "            imwrite(f\"{args.datadir}/augmented/{fname}\", img)\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"filename\": fname,\n",
    "                    \"semantic_label\": row[\"semantic_label\"],\n",
    "                    \"partition\": row[\"partition\"],\n",
    "                    \"numeric_label\": row[\"numeric_label\"],\n",
    "                    \"task\": row[\"task\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"input\", help=\"Path to input CSV file\")\n",
    "    parser.add_argument(\"datadir\", help=\"Data directory\", default=\"./data/\")\n",
    "    parser.add_argument(\n",
    "        \"-p\",\n",
    "        \"--partitions\",\n",
    "        nargs=\"+\",\n",
    "        help=\"Partitions (train|val|test|challenge|none)+ to apply augmentations to. Defaults to train\",\n",
    "        default=[\"train\"],\n",
    "    )\n",
    "    main(parser.parse_args(sys.argv[1:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EECS 445 - Introduction to Machine Learning\n",
    "Winter 2022 - Project 2\n",
    "Challenge\n",
    "    Constructs a pytorch model for a convolutional neural network\n",
    "    Usage: from model.challenge import Challenge\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "from utils import config\n",
    "\n",
    "\n",
    "class Challenge(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: define each layer of your network\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(\n",
    "            5, 5), stride=(2, 2), padding=2)\n",
    "        self.fc_1 = nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "\n",
    "\n",
    "        ##\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # TODO: initialize the parameters for your network\n",
    "        torch.manual_seed(42)\n",
    "        for conv in [self.conv1, self.conv2, self.conv3]:\n",
    "            C_in = conv.weight.size(1)\n",
    "            nn.init.normal_(conv.weight, 0.0, 1 / sqrt(5 * 5 * C_in))\n",
    "            nn.init.constant_(conv.bias, 0.0)\n",
    "\n",
    "        # TODO: initialize the parameters for [self.fc1]\n",
    "        nn.init.normal_(self.fc_1.weight, 0.0, 1 / sqrt(128))\n",
    "        nn.init.constant_(self.fc_1.bias, 0.0)\n",
    "\n",
    "        # nn.init.normal_(self.fc_2.weight, 0.0, 1 / sqrt(32))\n",
    "        # nn.init.constant_(self.fc_2.bias, 0.0)\n",
    "\n",
    "        # nn.init.normal_(self.fc_3.weight, 0.0, 1 / sqrt(32))\n",
    "        # nn.init.constant_(self.fc_3.bias, 0.0)\n",
    "\n",
    "        ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" You may optionally use the x.shape variables below to resize/view the size of \n",
    "            the input matrix at different points of the forward pass\n",
    "        \"\"\"\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # TODO: forward pass\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 128)\n",
    "        x = self.fc_1(x)\n",
    "\n",
    "        ##\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from dataset import get_train_val_test_loaders\n",
    "from model.target import Target\n",
    "from model.challenge import Challenge\n",
    "from train_common import *\n",
    "from utils import config\n",
    "import utils\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def freeze_layers(model, num_layers=0):\n",
    "    \"\"\"Stop tracking gradients on selected layers.\"\"\"\n",
    "    # TODO: modify model with the given layers frozen\n",
    "    #      e.g. if num_layers=2, freeze CONV1 and CONV2\n",
    "    #      Hint: https://pytorch.org/docs/master/notes/autograd.html\n",
    "\n",
    "    track = num_layers * 2\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if track == 0:\n",
    "            break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        track -= 1\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.requires_grad)\n",
    "\n",
    "\n",
    "def train(tr_loader, va_loader, te_loader, model, model_name, num_layers=0):\n",
    "    \"\"\"Train transfer learning model.\"\"\"\n",
    "    # TODO: define loss function, and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), weight_decay=0.01, lr=1e-3)\n",
    "    #\n",
    "\n",
    "    print(\"Loading target model with\", num_layers, \"layers frozen\")\n",
    "    model, start_epoch, stats = restore_checkpoint(model, model_name)\n",
    "\n",
    "    axes = utils.make_training_plot(\"Challenge Training\")\n",
    "\n",
    "    evaluate_epoch(\n",
    "        axes,\n",
    "        tr_loader,\n",
    "        va_loader,\n",
    "        te_loader,\n",
    "        model,\n",
    "        criterion,\n",
    "        start_epoch,\n",
    "        stats,\n",
    "        include_test=True,\n",
    "    )\n",
    "\n",
    "    # initial val loss for early stopping\n",
    "    global_min_loss = stats[0][1]\n",
    "\n",
    "    # TODO: patience for early stopping\n",
    "    patience = 10\n",
    "    curr_count_to_patience = 0\n",
    "    #\n",
    "\n",
    "    # Loop over the entire dataset multiple times\n",
    "    epoch = start_epoch\n",
    "    while curr_count_to_patience < patience:\n",
    "        # Train model\n",
    "        train_epoch(tr_loader, model, criterion, optimizer)\n",
    "\n",
    "        # Evaluate model\n",
    "        evaluate_epoch(\n",
    "            axes,\n",
    "            tr_loader,\n",
    "            va_loader,\n",
    "            te_loader,\n",
    "            model,\n",
    "            criterion,\n",
    "            epoch + 1,\n",
    "            stats,\n",
    "            include_test=True,\n",
    "        )\n",
    "\n",
    "        # Save model parameters\n",
    "        save_checkpoint(model, epoch + 1, model_name, stats)\n",
    "\n",
    "        curr_count_to_patience, global_min_loss = early_stopping(\n",
    "            stats, curr_count_to_patience, global_min_loss\n",
    "        )\n",
    "        epoch += 1\n",
    "\n",
    "    print(\"Finished Training\")\n",
    "\n",
    "    # Keep plot open\n",
    "    utils.save_tl_training_plot(num_layers)\n",
    "    utils.hold_training_plot()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Train transfer learning model and display training plots.\n",
    "\n",
    "    Train four different models with 4 layers frozen.\n",
    "    \"\"\"\n",
    "    # data loaders\n",
    "    if check_for_augmented_data(\"./data\"):\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\", batch_size=config(\"challenge.batch_size\"), augment=True\n",
    "        )\n",
    "    else:\n",
    "        tr_loader, va_loader, te_loader, _ = get_train_val_test_loaders(\n",
    "            task=\"target\",\n",
    "            batch_size=config(\"challenge.batch_size\"),\n",
    "        )\n",
    "\n",
    "    model = Challenge()\n",
    "\n",
    "    torch.nn.Dropout(p=0.5)\n",
    "    \n",
    "    freeze_three = copy.deepcopy(model)\n",
    "\n",
    "    freeze_layers(freeze_three, 3)\n",
    "    print(\"Loading source...\")\n",
    "    freeze_three, _, _ = restore_checkpoint(\n",
    "        freeze_three, config(\"source.checkpoint\"), force=True, pretrain=True\n",
    "    )\n",
    "\n",
    "    train(tr_loader, va_loader, te_loader,\n",
    "          freeze_three, \"./checkpoints/challenge3/\", 3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
