{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "i). <br />\n",
    "Mean: [124.495 118.847  95.293]<br />\n",
    "Std:  [62.754 59.597 62.425]<br /><br />\n",
    "ii). <br />\n",
    "Because we are training a model that is representative of the training dataset. This means that the model should be fit based on properties of the training data. We are simply applying the model to validation and testing data; so if we use their properties, it may cause overfitting. It is also te case that normalization puts everything on the same scale and this simplifies the learning process.\n",
    "\n",
    "### (b)\n",
    "![](Data_Preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Convolutional Neural Network\n",
    "\n",
    "### (a)\n",
    "For CNN, the number of learnable parameter (weights) can be calculated by (num_filters * filter_size * filter_size * num_channels) + num_filters <br />\n",
    "For fully connected layers, the number of weights is (input_size*output_size) + num_biases <br />\n",
    "\n",
    "So for layer 1, there are 16*5*5*3 + 16 = 1216 learnable parameters. <br />\n",
    "For layer 2, there are 64*5*5*16 + 64 = 25664 learnable parameters. <br />\n",
    "For layer 3, there are 8*5*5*64 + 6 = 12808 learnable parameters. <br />\n",
    "For the fully connected layer, there are 32*2 + 2 = 66 learnable parameters. <br />\n",
    "In total, there are 39754 parameters. <br />\n",
    "\n",
    "### (f)\n",
    "i). <br />\n",
    "One reason is that our the model is overfitting the training data so it won't perform well on the validation dataset. Another possible reason is that the the training, validation, and testing dataset aren't distributed similarly. So what the model learns from the training data can't be used to predict the validation data well. It is also possible that the amount of data we have isn't sufficient and this will lead to a model with poor performance. <br /><br />\n",
    "\n",
    "ii). <br />\n",
    "The model stopped training at epoch 11. With a patience of 10, the model would have to wait until epoch 16 to stop. Based on the training graphs, patience = 5 is a better choice because the graphs are all generally identical. However, patience = 5 takes fewer epoch to reach the performance. A higher patience might be better for training data that has a local minimum; if the patience is high, we won't stop at the local minimum and could potentially get a better model.\n",
    "![](cnn_training_plot_5.png)\n",
    "![](cnn_training_plot_10.png)\n",
    "\n",
    "iii). <br />\n",
    "The new size is 64x2x2 = 256. <br />\n",
    "\n",
    "|                \t| Epoch \t| Training AUROC \t| Validation AUROC \t|\n",
    "|---------------\t|-------\t|----------------\t|------------------\t|\n",
    "| 8 filters     \t| 6    \t| 0.9867         \t| 0.9419            \t|\n",
    "| 64 filters    \t| 2     \t| 0.9777         \t| 0.923           \t|\n",
    "\n",
    "The performance actually decreases as we increase the number of filters. This could be because that the architecture we set up makes the model overfits; we are having more neurons then we need to correctly classify an image. It could also be the case that the added inputs introduce unnecessary noise to the fully connected layer, which can distrub the overall accuracy of the model. <br />\n",
    "\n",
    "### (g)\n",
    "i). <br />\n",
    "|          \t| Training \t| Validation \t| Testing \t|\n",
    "|----------\t|----------\t|------------\t|---------\t|\n",
    "| Accuracy \t| 0.9667   \t| 0.9067       \t| 0.61    \t|\n",
    "| AUROC    \t| 0.9867   \t| 0.9419      \t| 0.6548  \t|\n",
    "\n",
    "ii). <br />\n",
    "Yes. We can see that the training accuracy is far better than the validation performance and it's similar for the AUROC score. So the model could potentially be overfitting <br />\n",
    "\n",
    "iii). <br />\n",
    "We see that there the testing perforamnce is much wrose than the validation performance in both the accuracy and AUROC score. This could be because we are not splitting our data properly and the validation data has a lot more label belonging to golden retriever while the testing data has a lot more labels of Collies. Therefore, the model will perform poorly on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Visualizing what the CNN has learned\n",
    "\n",
    "### (a) <br /> \n",
    "\n",
    "$$\\alpha_1 = \\frac{1}{16} \\sum_{i}\\sum_{j} \\frac{\\partial dy}{\\partial A'_{ij}} = 1+1+2+1+1+2+1+1-1+1-2-2=\\frac{3}{16}$$\n",
    "$$\\alpha_2 = \\frac{1}{16} \\sum_{i}\\sum_{j} \\frac{\\partial dy}{\\partial A'_{ij}} = 1+1+1+1+2+2+2+2+2+2+1-1-1-1=\\frac{7}{16}$$\n",
    "\n",
    "$$L'=ReLU(\\sum_{k}\\alpha'_kA^{(k)}) = ReLU(\\frac{3}{16}\\begin{bmatrix} 1 & 1 & 2 & 1 \\\\ 1 & 2 & 1 & 0 \\\\ 0 & 1 & 0 & -1 \\\\ 0 & 1 & -2 & -2 \\end{bmatrix} + \\frac{7}{16}\\begin{bmatrix} 1 & 1 & 1 & 1 \\\\ 2 & 2 & 2 & 2 \\\\ 2 & 2 & 1 & 0 \\\\ -1 & -1 & -1 & 0 \\end{bmatrix}) = \\frac{1}{16} \\begin{bmatrix} 10 & 10 & 13 & 10 \\\\ 17 & 20 & 17 & 14 \\\\ 14 & 17 & 7 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} $$\n",
    "\n",
    "### (b) <br /> \n",
    "The CNN appears to be using green pixels to identify between Golden Retrievers and Collies \n",
    "### (C) <br />\n",
    "Yes, it confirms the hypothesis. It seems like the model relies heavily on green pixels to identify between Golden Retrievers and Collies. However, we as humans know that this is not the distinctive feature between the two types of dogs. This shows that the model has a strong bias toward green color, and it further implies that the green is over-represented in the training data and our model is biased and cannot perform as well on the testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Transfer Learning & Data Augmentation\n",
    "\n",
    "### 4.1 Transfer Learning\n",
    "\n",
    "#### (c)\n",
    "![](source_training_plot.png)\n",
    "Epoch 10 has the lowest validation loss with a value of 1.8664. <br />\n",
    "\n",
    "#### (d)\n",
    "![](conf_matrix.png) <br />\n",
    "The classifier is the most accurate when predicting for Samoyed and it is the least accurate when predicting for Syberian Husky. This might be because we the training data we have is mislabeled and most Syberian Husky use Samoyed's label. This means that the classifier will treat most Syberian Husky as Samoyed and classify them correctly. Since Samoyed and Siberian Husky have very similar features, this strengthens the model's connection with Samoyed. On the other hand, the classifier doesn't have enough information to classify Syberian Husky because the train dataset doesn't contain many samples of it. This will result in very low accuracy for Syberian Husky. <br />\n",
    "\n",
    "#### (f)\n",
    "|                                                                  \t|        \t|  AUROC \t|        \t|\n",
    "|:----------------------------------------------------------------:\t|:------:\t|:------:\t|:------:\t|\n",
    "|                                                                  \t|  TRAIN \t|   VAL  \t|  TEST  \t|\n",
    "|            Freeze all CONV layers (Fine_tune FC layer)           \t| 0.9024 \t| 0.8983 \t| 0.8284 \t|\n",
    "| Freeze first two CONV layers (Fine-tune last CONV and FC layers) \t| 0.9756 \t| 0.9033 \t| 0.8072 \t|\n",
    "|   Freeze first CONV layer (Fine-tune last 2 conv, and fc layer)  \t| 0.9925 \t| 0.9237 \t| 0.7784 \t|\n",
    "|              Freeze no layers (Fine-tune all layers)             \t| 0.9905 \t| 0.9262 \t| 0.7576 \t|\n",
    "|    No pretraining or Transfer Learning (Section 2 performance)   \t| 0.9867 \t|  0.9419 \t| 0.6548 \t|\n",
    "\n",
    "Transfer learning helps significantly and the source task we used is very helpful because we witness a huge imporvement in testing performance. Freezing all layers results in a lot more epoches than when we freeze a subset of the layers. This is because without convolutional layers, we are not filtering our data. So our fully connected layer will receive many many more inputs. This means that it will take a lot longer to train the classifier and many epoches are taken before we reach a good performance. Nevertheless, freezing all convolutional layer results in a much better performance; this is because we are preserving all learnable features in our dataset. There will always be information loss during the filtering stage. The negative consequence of having no convolutional layer is high  computation cost. <br />\n",
    "\n",
    "### 4.2 Data Augmentation\n",
    "\n",
    "#### (b)\n",
    "\n",
    "|                                         \t|        \t|  AUROC \t|        \t|\n",
    "|:---------------------------------------:\t|:------:\t|:------:\t|:------:\t|\n",
    "|                                         \t|  TRAIN \t|   VAL  \t|  TEST  \t|\n",
    "|         Rotation (Keep original)        \t| 0.9864    | 0.9337    | 0.654    |\n",
    "|        Grayscale (keep original)        \t| 0.9914 \t| 0.9271 \t| 0.74 \t|\n",
    "|       Grayscale (discard original)      \t| 0.9667 \t| 0.7998 \t| 0.7736 \t|\n",
    "| No augmentation (section 2 performance) \t| 0.9867 \t|  0.9419 \t| 0.6548 \t|\n",
    "\n",
    "Training plot of Rotation (Keep original) <br />\n",
    "![](rotation_training_plot.png) <br />\n",
    "\n",
    "Training plot of Grayscale (keep original) <br />\n",
    "![](grayscale_true_training_plot.png) <br />\n",
    "\n",
    "Training plot of Grayscale (discard original) <br />\n",
    "![](grayscale_false_training_plot.png) <br />\n",
    "\n",
    "\n",
    "#### (c)\n",
    "When we apply rotation, the performance is roughly the same compared to when there is no augmentation. This might be because the validation and testing dataset doesn't contain many rotated objects. It's also possible that the rotation results in loss of information and the model is underfitted. One other possibility is that the augmentated samples aren't sufficient for the model to learn about potential rotated objects. <br />\n",
    "When we apply grayscale, either keeping or discarding the original images, we witness a great increase in testing performance. This could be due to the fact that grayscale essentially reduces the noise and thus prevents overfitting. However, one exception is that when we discard the original images, the model does worse on the validation data. This may be due to loss of information from the original dataset and insufficient training samples. However, the testing performance remains better that when we don't do any augmentation because of noise reduction. <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Regularization: I decided to apply Ridge Regularization by setting the weight decay to 0.01. I also added a dropout feature with a probability of 0.5. This "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
