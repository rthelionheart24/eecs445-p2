{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train...\n",
      "loading val...\n",
      "loading test...\n",
      "Train:\t 300\n",
      "Val:\t 150\n",
      "Test:\t 100\n",
      "Mean: [124.495 118.847  95.293]\n",
      "Std:  [62.754 59.597 62.425]\n"
     ]
    }
   ],
   "source": [
    "!python3 dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "i). <br />\n",
    "Mean: [124.495 118.847  95.293]<br />\n",
    "Std:  [62.754 59.597 62.425]<br /><br />\n",
    "ii). <br />\n",
    "Because we are training a model that is representative of the training dataset. This means that the model should be fit based on properties of the training data. We are simply applying the model to validation and testing data; so if we use their properties, it may cause overfitting.\n",
    "\n",
    "### (b)\n",
    "![](Data_Preprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### (a)\n",
    "There are 32*2 = 64 parameters to be learned; they are the weights of the fully connected layer.<br />\n",
    "\n",
    "### (f)\n",
    "i). <br />\n",
    "One reason is that our the model is overfitting the training data so it won't perform well on the validation dataset. Another possible reason is that the the training, validation, and testing dataset aren't distributed similarly. So what the model learns from the training data can't be used to predict the validation data well. <br /><br />\n",
    "\n",
    "ii). <br />\n",
    "The model stopped training at epoch 11. With a patience of 10, the model would have to wait until epoch 16 to stop. Based on the training graphs, patience = 5 is a better choice because the graphs are all generally identical. However, patience = 5 takes fewer epoch to reach the performance. A higher patience might be better for training data that has a local minimum; if the patience is high, we won't stop at the local minimum and could potentially get a better model.\n",
    "![](cnn_training_plot_5.png)\n",
    "![](cnn_training_plot_10.png)\n",
    "\n",
    "iii). <br />\n",
    "The new size is 64x2x2 = 256. <br />\n",
    "\n",
    "|                \t| Epoch \t| Training AUROC \t| Validation AUROC \t|\n",
    "|---------------\t|-------\t|----------------\t|------------------\t|\n",
    "| 8 filters     \t| 15    \t| 0.9991         \t| 0.942            \t|\n",
    "| 64 filters    \t| 4     \t| 0.9947         \t| 0.9271           \t|\n",
    "\n",
    "The performance actually decreases as we increase the number of filters. This could be because that the architecture we set up makes the model overfits; we are having more neurons then we need to correctly classify an image. It could also be the case that the added inputs introduce unnecessary noise to the fully connected layer, which can distrub the overall accuracy of the model. <br />\n",
    "\n",
    "### (g)\n",
    "i). <br />\n",
    "|          \t| Training \t| Validation \t| Testing \t|\n",
    "|----------\t|----------\t|------------\t|---------\t|\n",
    "| Accuracy \t| 0.9933   \t| 0.88       \t| 0.62    \t|\n",
    "| AUROC    \t| 0.9991   \t| 0.942      \t| 0.6996  \t|\n",
    "\n",
    "ii). <br />\n",
    "No, the validation performance is relatively close to training performance. <br />\n",
    "\n",
    "iii). <br />\n",
    "We see that there the testing perforamnce is much wrose than the validation performance in both the accuracy and AUROC score. This could be because we are not splitting our data properly and the validation data has a lot more label belonging to golden retriever while the testing data has a lot more labels of Collies. Therefore, the model will perform poorly  on the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing what the CNN has learned\n",
    "\n",
    "### (a) <br /> \n",
    "$$\\begin{equation} \n",
    "L^1 = \\begin{bmatrix} 3/16 & 0 & 3/8 & 5/8 \\\\ 0 & 0 & 3/16 & 0 \\\\ 0 & 3/16 & 0 & 1/4 \\\\ 7/16 & 5/8 & 1/2 & 1/2 \\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "### (b) <br /> \n",
    "The CNN appears to be using green pixels to identify between Golden Retrievers and Collies \n",
    "### (C) <br />\n",
    "Yes, it confirms the hypothesis. It seems like the model relies heavily on green pixels to identify between Golden Retrievers and Collies. However, we as humans know that this is not the distinctive feature between the two types of dogs. This shows that the model has a strong bias toward green color, and it further implies that the green is over-represented in the training data and our model is biased and cannpt perform as well on the testing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
